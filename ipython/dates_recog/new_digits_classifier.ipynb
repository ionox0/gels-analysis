{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x_train shape:', (60000, 28, 28, 1))\n",
      "(60000, 'train samples')\n",
      "(10000, 'test samples')\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s - loss: 0.3054 - acc: 0.9062 - val_loss: 0.0706 - val_acc: 0.9774\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.1053 - acc: 0.9689 - val_loss: 0.0486 - val_acc: 0.9838\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0809 - acc: 0.9764 - val_loss: 0.0423 - val_acc: 0.9865\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0681 - acc: 0.9797 - val_loss: 0.0402 - val_acc: 0.9867\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0591 - acc: 0.9822 - val_loss: 0.0349 - val_acc: 0.9883\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0538 - acc: 0.9841 - val_loss: 0.0321 - val_acc: 0.9890\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0485 - acc: 0.9857 - val_loss: 0.0334 - val_acc: 0.9878\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0458 - acc: 0.9865 - val_loss: 0.0291 - val_acc: 0.9905\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0427 - acc: 0.9877 - val_loss: 0.0299 - val_acc: 0.9898\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0402 - acc: 0.9882 - val_loss: 0.0295 - val_acc: 0.9903\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0383 - acc: 0.9890 - val_loss: 0.0279 - val_acc: 0.9908\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s - loss: 0.0363 - acc: 0.9888 - val_loss: 0.0274 - val_acc: 0.9912\n",
      "('Test loss:', 0.027435276504894135)\n",
      "('Test accuracy:', 0.99119999999999997)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./digits_cnn_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
